# RL AI Knight

### Abstract

1. 每次保存下来的5个.pt文件的作用
   - bestonline 大于100且是10的倍数的训练中，进行测试（关闭noise），选效果最好的
   - besttrainonline 在训练过程中选效果最好的
   - latestonline 最后一次训练结果，在训练中中途退出也是会保留的，下面两个不会
   - latestoptimizer 最后一次训练的optimizer结果
   - latesttarget0 最后一次训练的target_model结果
2. 游戏环境的区别：
   - V2
     - 攻击键自带初始惩罚2e-6
     - 有效攻击得到的奖励偏小(0.8->0.5)
     - gap略大0.05
     - 无事发生受到的惩罚偏小
     - 输入大小更大(160->192)
     - 长按按键时长更短更合理、长按失败的惩罚更小
     - reward计算上，不考虑用时
   - Survive
     - 只躲不打
     - reward计算上，只关心自身剩余血量
3. 代码版本的区别：
4. observation space 完全通过图像获得
   - 对截图的处理：
     - 先通过geo定位，然后截，但是**两侧截多了**，有效信息被截没了
     - 然后压缩成指定大小的灰度图
5. buffer是一个双端队列
6. exploration和epsilon decay区别

### ISSUE

1. 截图如何变成obs tuple
2. 评价指标：无效操作数量、成功主动躲避次数（问题是需要人为观察计算出的）
3. 收敛应该是会动作数减小，但是目前无效动作数还是过多
4. 泛化，考虑抽象BOSS形态与技能？
5. 使用一个环境，允许人类指导，但是是否就违背了强化学习初衷？
6. 频繁的操作会导致上一个状态的结果使下个状态该做出反应时无法进行
7. 研究高维动作空间：给出迭代次数下界
8. 图像怎么输入的？貌似是取屏幕的固定位置的像素？observation截图的频率是？计算速度够吗
9. LOSS的断层式增大？
10. replay buffer？
11. Q的更新周期是？每次动作？learning frequency？action frequency?
12. 如何寻找best model？（要最稳定、胜率最高而不是某次reward最大
13. V2比PER的改变？
14. double DQN？SVEA。。。
15. init后半段代码作用？

### TODO

1. ~~改reward函数，删除时间的奖励，先只追求成功率，后期再考虑速度~~
2. ~~增加对于胜利次数和分布的显示~~
3. ~~增加best更新时，记录下更新位置~~
4. ~~不用V2环境跑跑试试，包括选择hkenv 160*160，不用svea，修改save loc后缀~~
5. ~~尝试在soul warrior关卡训练，使用简单环境，修改后缀，不用epsilon decay~~
6. ~~给出一个评价指标：100次测试，比较成功率、平均reward，测一下179、513的best和besttrain~~
7. ~~用现成模型到新BOSS环境训练观察收敛速度，但是exploration用的是旧BOSS的，并测试对比效果~~
8. ~~用现成模型到新BOSS环境训练观察收敛速度，但是exploration是重新训练出来的，并测试对比效果~~
9. 既然exploration和model关联，那一起存进saved里吧
10. 很多小的改动都可以使收敛变快，但是如何定义收敛了
11. 研究下泛化的论文 
12. 比较hornet分支下的代码与当前区别并保留效果好的
13. buffer最后两个函数
14. 保存loss最小的模型
15. 拓展动作状态空间，加入冲刺

### LOG

1. 630 原版代码

2. 015 加上赢输奖励，没有太大提升，仍然几乎是闭眼乱打，守株待兔一直冲墙砍，可能是过拟合

3. 388 效果最好的一次，550次迭代、使用epsilon decay函数为100 / steps

4. 970 改为1000次迭代，函数也改为1000 / steps

5. 909 实现了1 2 3，但是没找到LOSS不降反升的原因

6. 179 实现了4

7. 988 实现了4同时取消了epsilon decay但是exploration没变因此无效

8. 082 实现5

9. 513 实现了4同时取消了epsilon decay

10. 实现了6

    |      | besttrain    | best         |
    | ---- | ------------ | ------------ |
    | 179  | 24.382918 98 | 24.449716 98 |
    | 513  | 23.483529 93 | 21.769154 82 |

11. 894 实现了7

    |      | besttrain    | best         |
    | ---- | ------------ | ------------ |
    | 894  | 4.409176 3   | 4.239348 3   |
    | 178  | 14.745453 57 | 14.114856 56 |

12. 164 实现了8

    |      | besttrain    | best         |
    | ---- | ------------ | ------------ |
    | 164  | 6.021111 3   | 7.346920 7   |
    | 178  | 14.056264 51 | 14.678022 63 |

